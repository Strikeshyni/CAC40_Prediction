# Data Ingestion Layer â€” Architecture Overview

> **CAC40_Prediction Multimodal ML Pipeline**  
> **Module:** `data/ingestion/`  
> **Version:** 1.0.0  
> **Last Updated:** January 2026

---

## ğŸ“ Directory Structure

```
data/ingestion/
â”‚
â”œâ”€â”€ __init__.py                    # Module initialization
â”œâ”€â”€ README.md                      # This file
â”‚
â”œâ”€â”€ market_data/                   # OHLCV Price Data
â”‚   â”œâ”€â”€ SPECS.md                   # Technical specifications
â”‚   â”œâ”€â”€ fetchers/                  # Data fetcher implementations
â”‚   â”‚   â”œâ”€â”€ yfinance_fetcher.py
â”‚   â”‚   â”œâ”€â”€ polygon_fetcher.py
â”‚   â”‚   â””â”€â”€ base_fetcher.py
â”‚   â”œâ”€â”€ processors/                # Data validation & transformation
â”‚   â”‚   â”œâ”€â”€ validator.py
â”‚   â”‚   â””â”€â”€ weekly_aggregator.py
â”‚   â”œâ”€â”€ storage/                   # Parquet writers
â”‚   â”‚   â””â”€â”€ storage_manager.py
â”‚   â””â”€â”€ raw/                       # Raw data storage (git-ignored)
â”‚       â””â”€â”€ daily/
â”‚
â”œâ”€â”€ news_feeds/                    # Financial News & Sentiment
â”‚   â”œâ”€â”€ SPECS.md                   # Technical specifications
â”‚   â”œâ”€â”€ fetchers/                  # API fetchers
â”‚   â”‚   â”œâ”€â”€ fmp_fetcher.py
â”‚   â”‚   â”œâ”€â”€ newsapi_fetcher.py
â”‚   â”‚   â””â”€â”€ rss_aggregator.py
â”‚   â”œâ”€â”€ scrapers/                  # Web scrapers
â”‚   â”‚   â”œâ”€â”€ yahoo_finance.py
â”‚   â”‚   â”œâ”€â”€ investing_com.py
â”‚   â”‚   â””â”€â”€ boursorama.py
â”‚   â”œâ”€â”€ processors/                # Text processing
â”‚   â”‚   â”œâ”€â”€ text_extractor.py
â”‚   â”‚   â”œâ”€â”€ ticker_extractor.py
â”‚   â”‚   â””â”€â”€ timestamp_normalizer.py
â”‚   â””â”€â”€ raw/                       # Raw data storage
â”‚
â”œâ”€â”€ corporate_filings/             # Regulatory Filings
â”‚   â”œâ”€â”€ SPECS.md                   # Technical specifications
â”‚   â”œâ”€â”€ fetchers/                  # Filing fetchers
â”‚   â”‚   â”œâ”€â”€ sec_edgar_fetcher.py
â”‚   â”‚   â”œâ”€â”€ esef_fetcher.py
â”‚   â”‚   â””â”€â”€ amf_fetcher.py
â”‚   â”œâ”€â”€ parsers/                   # XBRL/HTML parsers
â”‚   â”‚   â”œâ”€â”€ xbrl_parser.py
â”‚   â”‚   â””â”€â”€ sec_facts_parser.py
â”‚   â”œâ”€â”€ scrapers/                  # Fallback scrapers
â”‚   â”‚   â””â”€â”€ yahoo_financials_scraper.py
â”‚   â””â”€â”€ raw/                       # Raw filings storage
â”‚
â””â”€â”€ macro_indicators/              # Macroeconomic Data
    â”œâ”€â”€ SPECS.md                   # Technical specifications
    â”œâ”€â”€ fetchers/                  # Data fetchers
    â”‚   â”œâ”€â”€ fred_fetcher.py
    â”‚   â””â”€â”€ ecb_fetcher.py
    â”œâ”€â”€ processors/                # Feature engineering
    â”‚   â”œâ”€â”€ frequency_aligner.py
    â”‚   â”œâ”€â”€ feature_engineer.py
    â”‚   â””â”€â”€ yield_curve.py
    â””â”€â”€ raw/                       # Raw data storage
```

---

## ğŸ¯ Module Objectives

| Module | Primary Goal | Key Sources |
|--------|--------------|-------------|
| **Market Data** | Adjusted OHLCV for price prediction | Yahoo Finance, Polygon.io |
| **News Feeds** | Sentiment signals & event detection | FMP, NewsAPI, Yahoo RSS |
| **Corporate Filings** | Fundamental data (revenue, EPS) | SEC EDGAR, Yahoo Financials |
| **Macro Indicators** | Economic regime features | FRED, ECB |

---

## ğŸ”§ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements_ingestion.txt
```

### 2. Configure API Keys

Create a `.env` file in the project root:

```env
# Market Data
POLYGON_API_KEY=your_polygon_key

# News
FMP_API_KEY=your_fmp_key
NEWSAPI_KEY=your_newsapi_key

# Macro
FRED_API_KEY=your_fred_key

# SEC (email for User-Agent)
SEC_USER_EMAIL=your@email.com
```

### 3. Run Individual Modules

```python
# Market Data
from data.ingestion.market_data.fetchers.yfinance_fetcher import YFinanceFetcher

fetcher = YFinanceFetcher()
df = fetcher.fetch(tickers=["AAPL", "MC.PA"], start="2020-01-01")

# Macro Indicators
from data.ingestion.macro_indicators.fetchers.fred_fetcher import FREDFetcher

fred = FREDFetcher()
macro_data = fred.fetch_multiple(series_ids=["DGS10", "CPIAUCSL"])
```

---

## ğŸ“Š Data Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA INGESTION PIPELINE                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ Market Data  â”‚   â”‚  News Feeds  â”‚   â”‚   Corporate Filings      â”‚   â”‚
â”‚   â”‚   (OHLCV)    â”‚   â”‚  (Articles)  â”‚   â”‚   (10-K, 10-Q, XBRL)     â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚          â”‚                  â”‚                        â”‚                  â”‚
â”‚          â–¼                  â–¼                        â–¼                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚                    RAW DATA STORAGE                          â”‚     â”‚
â”‚   â”‚                (JSONL / Parquet / HTML)                      â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                              â”‚                                          â”‚
â”‚                              â–¼                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚                    PROCESSING LAYER                          â”‚     â”‚
â”‚   â”‚  â€¢ Validation  â€¢ Normalization  â€¢ Feature Engineering        â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                              â”‚                                          â”‚
â”‚                              â–¼                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚                  PROCESSED DATA STORAGE                      â”‚     â”‚
â”‚   â”‚              (Partitioned Parquet Files)                     â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                              â”‚                                          â”‚
â”‚                              â–¼                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚   â”‚                   ML FEATURE STORE                           â”‚     â”‚
â”‚   â”‚           (Ready for Model Training)                         â”‚     â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                         â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                     â”‚
â”‚   â”‚Macro Indicatorsâ”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚   â”‚(FRED, ECB)     â”‚                                                   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                   â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“… Scheduling Strategy

| Data Type | Update Frequency | Trigger Time (UTC) |
|-----------|------------------|-------------------|
| OHLCV (Daily) | Daily | 22:00 (after US close) |
| OHLCV (Weekly) | Weekly (Friday) | 22:00 |
| News APIs | Every 4 hours | 00:00, 04:00, 08:00, ... |
| News RSS | Every 30 minutes | Continuous |
| SEC Filings | Daily | 08:00 |
| FRED Data | Daily | 08:00 |
| ECB Data | Daily | 10:00 |

---

## ğŸ—„ï¸ Storage Strategy

### Format: **Parquet** (Primary)

```python
# Partitioning strategy
df.to_parquet(
    "data/processed/",
    partition_cols=["ticker", "year"],
    engine="pyarrow",
    compression="snappy"
)
```

### Advantages over CSV:
- 5-10x compression
- Columnar storage (fast queries)
- Schema enforcement
- Native pandas integration

### Backup: **JSONL** (Raw responses)

```python
# For API responses
with open("raw/2026-01-26.jsonl", "a") as f:
    f.write(json.dumps(response) + "\n")
```

---

## ğŸ”’ Rate Limit Summary

| Source | Limit | Strategy |
|--------|-------|----------|
| Yahoo Finance | ~2000/hour (unofficial) | 0.5s delay, batch requests |
| Polygon.io | Plan-based | Respect headers |
| FMP API | 250-750/day (free) | Prioritize, cache |
| NewsAPI | 100/day (free) | Reserve for key queries |
| SEC EDGAR | 10/second | 0.2s delay, include email |
| FRED | 120/minute | 0.5s delay |
| ECB | No limit | 1s delay (courtesy) |

---

## ğŸ§ª Testing

Run module tests:

```bash
# Test all ingestion modules
pytest data/ingestion/tests/ -v

# Test specific module
pytest data/ingestion/market_data/tests/ -v
```

---

## ğŸ“š Detailed Specifications

Each module contains a `SPECS.md` file with:
- Objective & scope
- Library recommendations
- Data source details (URLs, rate limits)
- Schema definitions
- Implementation plan
- Sample code

**See:**
- [Market Data SPECS](market_data/SPECS.md)
- [News Feeds SPECS](news_feeds/SPECS.md)
- [Corporate Filings SPECS](corporate_filings/SPECS.md)
- [Macro Indicators SPECS](macro_indicators/SPECS.md)

---

## ğŸš€ Next Steps

1. **Phase 1:** Implement Market Data fetchers (Priority: yfinance)
2. **Phase 2:** Implement FRED/ECB macro fetchers
3. **Phase 3:** Build news aggregation pipeline
4. **Phase 4:** Implement SEC EDGAR integration
5. **Phase 5:** Create unified orchestrator

---

*Authored for CAC40_Prediction ML Pipeline â€” Data Ingestion Layer*
